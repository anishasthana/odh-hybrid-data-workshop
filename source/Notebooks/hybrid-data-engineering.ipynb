{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Data Hub and Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data engineers/scientist can use Open Data Hub and object storage, specifically, Ceph object storage, much in the same way they are accoustomed to interacting with Amazon Simple Storage Service (S3). This is made possible because Ceph's object storage gateway offers excellent fidelity with the modalities of Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. Amoung the services it provides interfaces for is Amazon S3. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://172.30.170.19:8000\n",
      "user8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3_endpoint_url = os.environ['S3_ENDPOINT_URL']\n",
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "s3_bucket_name = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "print(s3_endpoint_url)\n",
    "print(s3_bucket_name)\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= s3_endpoint_url,\n",
    "                       aws_access_key_id = s3_access_key,\n",
    "                       aws_secret_access_key = s3_secret_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a bucket, uploading and object (put), and listing the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kube-metrics/_SUCCESS\n",
      "kube-metrics/part-00000-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00001-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00002-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00003-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00004-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00005-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00006-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00007-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00008-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00009-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00010-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00011-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00012-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00013-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00014-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00015-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00016-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00017-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00018-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00019-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00020-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00021-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00022-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00023-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00024-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00025-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00026-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00027-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00028-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00029-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00030-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00031-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00032-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00033-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00034-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00035-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00036-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00037-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00038-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00039-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00040-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00041-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00042-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00043-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00044-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00045-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00046-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00047-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00048-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00049-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00050-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00051-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00052-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00053-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00054-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00055-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00056-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00057-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00058-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00059-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00060-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00061-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00062-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00063-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00064-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00065-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00066-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00067-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00068-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00069-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00070-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00071-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00072-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00073-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00074-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "kube-metrics/part-00075-1a71eceb-3f41-4255-a753-32cfeb4d3ede-c000.json.bz2\n",
      "object\n",
      "trip_report.tsv/_SUCCESS\n",
      "trip_report.tsv/part-00000-4ab5872b-cee0-47d4-8afa-a480cc79ce3f-c000.csv\n"
     ]
    }
   ],
   "source": [
    "s3.create_bucket(Bucket=s3_bucket_name)\n",
    "s3.put_object(Bucket=s3_bucket_name,Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket=s3_bucket_name)['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Data Hub operator will install Spark. Each Jupyterhub user will also have a dedicated Spark cluster (Master and Workers) to use. First step is to connect to the Spark Cluster and get a spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "##################################################\n",
    "# This is a HACK until we can resolve the issue\n",
    "# preventing the spark nodes from resolving pod\n",
    "# names\n",
    "spark_driver_host = \"10.128.2.70\"\n",
    "##################################################\n",
    "\n",
    "#os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--conf spark.driver.host={spark_driver_host} --conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--conf spark.driver.host={spark_driver_host} --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5 pyspark-shell\"\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "spark = SparkSession.builder.master(spark_cluster_url).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-cluster-user8-w-gkwjn', 'spark-cluster-user8-w-b58cp']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import socket\n",
    "spark.range(100, numPartitions=100).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.read.text(f\"s3a://{s3_bucket_name}/object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0\n",
    "print(\"Total number of rows in df0: %d\" % df0.count())\n",
    "df0.printSchema()\n",
    "df0.show()\n",
    "df0.select(\"value\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Hybrid Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Hadoop 2.8, S3A supports per bucket configuration. This is very powerful. It allows us to have a distinct S3A configuration, with a different endpoint and different set of credentials. With this I can use a single Spark context to read a parquet file from a bucket in the public cloud (Amazon S3) into a data frame, then turn around and write that dataframe as a parquet file into a bucket that exists in the Ceph(Rook) local cluster installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Public to Private ETL__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply read tab separated data from a bucket in Amazon S3 and write it back out to a bucket in our Ceph(Rook) service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in df0: 135\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n",
      "+------------------+---------------+--------------------+--------------------+--------------------+--------------------+---------------+----------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+-----------+--------------+--------------------+-----------+----+\n",
      "|               _c0|            _c1|                 _c2|                 _c3|                 _c4|                 _c5|            _c6|       _c7|         _c8|                 _c9|                _c10|     _c11|                _c12|                _c13|                _c14|       _c15|          _c16|                _c17|       _c18|_c19|\n",
      "+------------------+---------------+--------------------+--------------------+--------------------+--------------------+---------------+----------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+-----------+--------------+--------------------+-----------+----+\n",
      "|         Timestamp|      Your Name|Number of custome...|Primary Audience ...|Primary Sales Con...|Key People (Prese...|        Outcome|Full Notes|   Trip Name|       Email Address|        Action Items|     Date|          Highlights|           Lowlights|           Learnings|Trip Region|Number of Days|Estimated Cost (USD)|Product Mix|null|\n",
      "|1/24/2017 16:46:47|  Bryanty Pople|                  35|            Customer|   Vertical Solution|    Emera Ambrogioli|     Successful|      null|       Yodel|eambrogioli0@cons...|Organized leading...|1/24/2017|Cras in purus eu ...|In quis justo. Ma...|In quis justo. Ma...|         NA|             1|                 200|       null|null|\n",
      "| 2/1/2017 12:09:48|  Bryanty Pople|                   1|               Event|     Solution (ISBU)|     Rurik Ossipenko|   Unsuccessful|      null|       Tazzy|rossipenko1@angel...|Digitized bifurca...|1/16/2017|Nullam sit amet t...|Phasellus sit ame...|Sed sagittis. Nam...|         NA|             5|                 200|       null|null|\n",
      "|  2/3/2017 9:16:38|  Bryanty Pople|                   3|               Field|     Solution (ISBU)|       Sheryl Gayter|   Unsuccessful|      null|   Browsecat|sgayter2@friendfe...|Down-sized discre...|1/31/2017|Mauris enim leo, ...|Etiam vel augue. ...|In sagittis dui v...|       EMEA|             3|                 200|       null|null|\n",
      "| 2/5/2017 13:41:22|  Bryanty Pople|                  30|         Engineering|                RHEL|    Minda Djurdjevic|     Successful|      null|       Oodoo| mdjurdjevic3@goo.gl|Expanded bifurcat...|1/25/2017|Duis at velit eu ...|Mauris sit amet e...|Nullam porttitor ...|       EMEA|             2|                2000|       null|null|\n",
      "| 2/5/2017 13:47:19|  Bryanty Pople|                1500|               Event|          Containers|      Sondra Heasley|     Successful|      null|Thoughtworks|sheasley4@google....|Stand-alone unifo...|1/27/2017|Sed sagittis. Nam...|Nulla neque liber...|Proin leo odio, p...|       EMEA|             3|                1000|       null|null|\n",
      "| 2/5/2017 14:00:47|  Bryanty Pople|                  15|            Customer|          Containers|       Gerda Bussens|     Successful|      null|     Skalith|gbussens5@harvard...|Balanced neutral ...| 2/2/2017|Integer pede just...|In sagittis dui v...|Integer ac leo. P...|       EMEA|             1|                1000|       null|null|\n",
      "| 2/5/2017 14:07:13|  Jocko Stanion|                  20|            Customer|          Containers|  Willamina Handrock|     Successful|      null|    Photobug|   whandrock6@un.org|Cross-platform di...| 2/2/2017|Suspendisse poten...|Vivamus vestibulu...|Phasellus in feli...|       EMEA|             1|                1000|       null|null|\n",
      "| 2/5/2017 14:10:40|     Tabb Rixon|                   3|            Customer|          Containers|      Ralf Tomkowicz|Partial Success|      null|      Trudeo| rtomkowicz7@who.int|Advanced transiti...| 2/3/2017|In blandit ultric...|Sed sagittis. Nam...|Vestibulum quam s...|       EMEA|             1|                1000|       null|null|\n",
      "| 2/5/2017 14:13:55|     Tabb Rixon|                5000|               Event|          Containers|      Sherry Caddell|     Successful|      null|Jabbersphere| scaddell8@google.nl|De-engineered dem...| 2/4/2017|Mauris ullamcorpe...|Phasellus sit ame...|Proin eu mi. Null...|       EMEA|             2|                1500|       null|null|\n",
      "| 2/8/2017 16:34:08|     Tabb Rixon|                   1|             Analyst|                 IoT|      Rolando Elrick|Partial Success|      null|        Latz|    relrick9@free.fr|Extended backgrou...|1/27/2017|Cras mi pede, mal...|Maecenas tincidun...|Proin interdum ma...|       EMEA|             0|                 200|       null|null|\n",
      "| 2/8/2017 16:39:21|     Tabb Rixon|                   3|             Analyst|                 IoT|           Viv Heibl|     Successful|      null|       Quatz|  vheibla@scribd.com|Focused secondary...| 1/5/2017|Etiam justo. Etia...|Curabitur in libe...|Sed ante. Vivamus...|         NA|             0|                 200|       null|null|\n",
      "|2/10/2017 13:35:53|     Tabb Rixon|                   1|            Customer|            Security|      Johan Cochrane|Partial Success|      null|    Fanoodle|jcochraneb@umich.edu|Managed radical n...|2/10/2017|Proin leo odio, p...|Donec semper sapi...|Duis consequat du...|       Call|             0|                 200|       null|null|\n",
      "|2/13/2017 11:42:57|     Tabb Rixon|                  10|               Field|     Solution (ISBU)|        Cynde Lorans|   Unsuccessful|      null|  Topicstorm|   cloransc@furl.net|Universal executi...| 2/9/2017|In est risus, auc...|Praesent id massa...|Aenean fermentum....|       EMEA|             1|                 200|       null|null|\n",
      "| 3/6/2017 11:34:14|     Tabb Rixon|                   3|            Customer|          Containers|Collette Landsbor...|     Successful|      null|       Abatz|clandsboroughd@om...|Down-sized encomp...| 3/1/2017|Mauris enim leo, ...|    Quisque ut erat.|Nulla ut erat id ...|         NA|             1|                1000|       null|null|\n",
      "| 3/6/2017 11:46:20|     Tabb Rixon|                   3|            Customer|          Containers|      Zollie Frodsam|     Successful|      null|       Mydeo| zfrodsame@jigsy.com|Function-based ba...| 3/1/2017|In blandit ultric...|Morbi sem mauris,...|Mauris enim leo, ...|         NA|             1|                 500|       null|null|\n",
      "| 3/6/2017 12:08:08|Kaitlynn Reding|                  27|               Event|          Containers| Dominick Aleksashin|     Successful|      null|      Rhynyx|  daleksashinf@de.vu|Profound multi-ta...| 3/1/2017|Etiam vel augue. ...|Curabitur convallis.|Sed sagittis. Nam...|         NA|             1|                1000|       null|null|\n",
      "| 3/6/2017 12:34:27|Kaitlynn Reding|                   1|            Customer|          Containers|       Darcy Mallock|   Unsuccessful|      null|      Divavu|    dmallockg@360.cn|Configurable web-...| 3/2/2017|Donec diam neque,...|Etiam vel augue. ...|Morbi non lectus....|         NA|             1|                1000|       null|null|\n",
      "| 3/15/2017 6:08:02|Kaitlynn Reding|                 500|               Field|     Solution (ISBU)|  Bendite Vercruysse|   Unsuccessful|      null|      Agimba|bvercruysseh@dmoz...|Innovative backgr...|2/28/2017|Aliquam erat volu...|Vestibulum ante i...|Nullam porttitor ...|       APAC|            12|                3000|       null|null|\n",
      "|3/18/2017 19:57:10|Kaitlynn Reding|                  50|         Engineering|          Containers|    Franciskus Ather|Partial Success|      null|    Linktype|fatheri@google.co...|Persistent system...| 3/7/2017|Curabitur gravida...|Integer ac leo. P...|Aenean fermentum....|         NA|             3|                1200|       null|null|\n",
      "+------------------+---------------+--------------------+--------------------+--------------------+--------------------+---------------+----------+------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+-----------+--------------+--------------------+-----------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tripreport = spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\")\n",
    "print(\"Total number of rows in df0: %d\" % tripreport.count())\n",
    "tripreport.printSchema()\n",
    "tripreport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.csv(f\"s3a://{s3_bucket_name}/trip_report.tsv\",sep=\"\\t\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all JSON files from a bucket prefix (pseudo directory) in Amazon S3 and write them back out to a bucket in our Ceph(Rook) service with the same bucket prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in df0: 26775\n",
      "root\n",
      " |-- metric: struct (nullable = true)\n",
      " |    |-- __name__: string (nullable = true)\n",
      " |    |-- beta_kubernetes_io_arch: string (nullable = true)\n",
      " |    |-- beta_kubernetes_io_fluentd_ds_ready: string (nullable = true)\n",
      " |    |-- beta_kubernetes_io_instance_type: string (nullable = true)\n",
      " |    |-- beta_kubernetes_io_os: string (nullable = true)\n",
      " |    |-- clam_controller_enabled: string (nullable = true)\n",
      " |    |-- clam_server_enabled: string (nullable = true)\n",
      " |    |-- failure_domain_beta_kubernetes_io_region: string (nullable = true)\n",
      " |    |-- failure_domain_beta_kubernetes_io_zone: string (nullable = true)\n",
      " |    |-- fluentd_test: string (nullable = true)\n",
      " |    |-- hostname: string (nullable = true)\n",
      " |    |-- image_inspector_enabled: string (nullable = true)\n",
      " |    |-- instance: string (nullable = true)\n",
      " |    |-- job: string (nullable = true)\n",
      " |    |-- kubernetes_io_hostname: string (nullable = true)\n",
      " |    |-- logging_infra_fluentd: string (nullable = true)\n",
      " |    |-- node_role_kubernetes_io_compute: string (nullable = true)\n",
      " |    |-- node_role_kubernetes_io_master: string (nullable = true)\n",
      " |    |-- operation_type: string (nullable = true)\n",
      " |    |-- ops_node: string (nullable = true)\n",
      " |    |-- placement: string (nullable = true)\n",
      " |    |-- quantile: string (nullable = true)\n",
      " |    |-- region: string (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|              metric|              values|\n",
      "+--------------------+--------------------+\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, 178...|\n",
      "|[kubelet_docker_o...|[[1524614399, 178...|\n",
      "|[kubelet_docker_o...|[[1524614399, 178...|\n",
      "|[kubelet_docker_o...|[[1524614399, 468...|\n",
      "|[kubelet_docker_o...|[[1524614399, 108...|\n",
      "|[kubelet_docker_o...|[[1524614399, 243...|\n",
      "|[kubelet_docker_o...|[[1524614399, 473...|\n",
      "|[kubelet_docker_o...|[[1524614399, 804...|\n",
      "|[kubelet_docker_o...|[[1524614399, 115...|\n",
      "|[kubelet_docker_o...|[[1524614399, 125...|\n",
      "|[kubelet_docker_o...|[[1524614399, 174...|\n",
      "|[kubelet_docker_o...|[[1524614399, 910...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "|[kubelet_docker_o...|[[1524614399, NaN...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonFile= spark.read.option(\"multiline\", True).option(\"mode\", \"PERMISSIVE\").json(\"s3a://bd-dist/kube-metrics\")\n",
    "print(\"Total number of rows in df0: %d\" % jsonFile.count())\n",
    "jsonFile.printSchema()\n",
    "jsonFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Prometheus data set from Ceph(Rook) into a data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import statistics libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Display schema of files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display schema:')\n",
    "jsonFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query the JSON data using filters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|              values|   operation_type|\n",
      "+--------------------+-----------------+\n",
      "|[[1524614399, NaN...|             info|\n",
      "|[[1524614399, NaN...|inspect_container|\n",
      "|[[1524614399, 178...|    inspect_image|\n",
      "|[[1524614399, 108...|  list_containers|\n",
      "|[[1524614399, 804...|      list_images|\n",
      "|[[1524614399, 174...|          version|\n",
      "|[[1524527999, NaN...|             info|\n",
      "|[[1524527999, NaN...|inspect_container|\n",
      "|[[1524527999, 206...|    inspect_image|\n",
      "|[[1524527999, 107...|  list_containers|\n",
      "|[[1524527999, 796...|      list_images|\n",
      "|[[1524527999, 163...|          version|\n",
      "|[[1524182399, NaN...|             info|\n",
      "|[[1524182399, NaN...|inspect_container|\n",
      "|[[1524182399, 246...|    inspect_image|\n",
      "|[[1524182399, 105...|  list_containers|\n",
      "|[[1524182399, 692...|      list_images|\n",
      "|[[1524182399, 162...|          version|\n",
      "|[[1524441599, NaN...|             info|\n",
      "|[[1524441599, NaN...|inspect_container|\n",
      "+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- values: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- operation_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Register the created SchemaRDD as a temporary table.\n",
    "jsonFile.registerTempTable(\"kubelet_docker_operations_latency_microseconds\")\n",
    "\n",
    "#Filter the results into a data frame\n",
    "data = spark.sql(\"SELECT values, metric.operation_type FROM kubelet_docker_operations_latency_microseconds WHERE metric.quantile='0.9' AND metric.hostname='free-stg-master-03fb6'\")\n",
    "\n",
    "data.show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>operation_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>1520872259</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>1520872319</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:31:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>1520872379</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:32:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>1520872439</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:33:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>1520872499</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:34:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    utc_timestamp  value    operation_type           timestamp\n",
       "987    1520872259  55420  remove_container 2018-03-12 16:30:59\n",
       "988    1520872319  55420  remove_container 2018-03-12 16:31:59\n",
       "989    1520872379  55420  remove_container 2018-03-12 16:32:59\n",
       "990    1520872439  55420  remove_container 2018-03-12 16:33:59\n",
       "991    1520872499  55420  remove_container 2018-03-12 16:34:59"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd = data.toPandas()\n",
    "\n",
    "\n",
    "\n",
    "OP_TYPE = 'list_images'\n",
    "\n",
    "df2 = pd.DataFrame(columns = ['utc_timestamp','value', 'operation_type'])\n",
    "#df2 ='\n",
    "for op in set(data_pd['operation_type']):\n",
    "    dict_raw = data_pd[data_pd['operation_type'] == op]['values']\n",
    "    list_raw = []\n",
    "    for key in dict_raw.keys():\n",
    "        list_raw.extend(dict_raw[key])\n",
    "    temp_frame = pd.DataFrame(list_raw, columns = ['utc_timestamp','value'])\n",
    "    temp_frame['operation_type'] = op\n",
    "    \n",
    "    df2 = df2.append(temp_frame)\n",
    "\n",
    "\n",
    "df2 = df2[df2['value'] != 'NaN']\n",
    "\n",
    "df2['value'] = df2['value'].apply(lambda a: int(a))\n",
    "\n",
    "df2['timestamp'] = df2['utc_timestamp'].apply(lambda a : datetime.fromtimestamp(int(a)))\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Objective - Verify Above Alerts __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store timestamp with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utc_timestamp</th>\n",
       "      <th>value</th>\n",
       "      <th>operation_type</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1520872259</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:30:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1520872319</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:31:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1520872379</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:32:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1520872439</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:33:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1520872499</td>\n",
       "      <td>55420</td>\n",
       "      <td>remove_container</td>\n",
       "      <td>2018-03-12 16:34:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  utc_timestamp  value    operation_type           timestamp\n",
       "0    1520872259  55420  remove_container 2018-03-12 16:30:59\n",
       "1    1520872319  55420  remove_container 2018-03-12 16:31:59\n",
       "2    1520872379  55420  remove_container 2018-03-12 16:32:59\n",
       "3    1520872439  55420  remove_container 2018-03-12 16:33:59\n",
       "4    1520872499  55420  remove_container 2018-03-12 16:34:59"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.reset_index(inplace =True)\n",
    "\n",
    "del df2['index']\n",
    "\n",
    "df2['operation_type'].unique()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe in local Ceph(Rook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- utc_timestamp: string (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      " |-- operation_type: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame to Spark DataFrame\n",
    "dfSpark = spark.createDataFrame(df2)\n",
    "dfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSpark.write.csv(f\"s3a://{s3_bucket_name}//kube-metrics/operationinfo.csv\",sep=\"\\t\",mode=\"overwrite\",header = 'True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just for us testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      "\n",
      "+-------------+-----+----------------+--------------------+\n",
      "|          _c0|  _c1|             _c2|                 _c3|\n",
      "+-------------+-----+----------------+--------------------+\n",
      "|utc_timestamp|value|  operation_type|           timestamp|\n",
      "|   1520872259|55420|remove_container|2018-03-12T16:30:...|\n",
      "|   1520872319|55420|remove_container|2018-03-12T16:31:...|\n",
      "|   1520872379|55420|remove_container|2018-03-12T16:32:...|\n",
      "|   1520872439|55420|remove_container|2018-03-12T16:33:...|\n",
      "|   1520872499|55420|remove_container|2018-03-12T16:34:...|\n",
      "|   1520872559|55420|remove_container|2018-03-12T16:35:...|\n",
      "|   1520872619|55420|remove_container|2018-03-12T16:36:...|\n",
      "|   1520872679|55420|remove_container|2018-03-12T16:37:...|\n",
      "|   1520872739|55420|remove_container|2018-03-12T16:38:...|\n",
      "|   1520872919|33524|remove_container|2018-03-12T16:41:...|\n",
      "|   1520872979|33524|remove_container|2018-03-12T16:42:...|\n",
      "|   1520873039|17815|remove_container|2018-03-12T16:43:...|\n",
      "|   1520873099|17815|remove_container|2018-03-12T16:44:...|\n",
      "|   1520873159|17815|remove_container|2018-03-12T16:45:...|\n",
      "|   1520873219|17815|remove_container|2018-03-12T16:46:...|\n",
      "|   1520873279|17815|remove_container|2018-03-12T16:47:...|\n",
      "|   1520873339|17815|remove_container|2018-03-12T16:48:...|\n",
      "|   1520873399|17815|remove_container|2018-03-12T16:49:...|\n",
      "|   1520873459|17815|remove_container|2018-03-12T16:50:...|\n",
      "+-------------+-----+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testdf = spark.read.csv(f\"s3a://{s3_bucket_name}//kube-metrics/operationinfo.csv\",sep=\"\\t\")\n",
    "testdf.printSchema()\n",
    "testdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
